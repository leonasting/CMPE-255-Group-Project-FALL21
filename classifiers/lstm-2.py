# -*- coding: utf-8 -*-
"""dl_update.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i1ZQpxVwu0ovYo1Hkl6d6I-UlGW_G5S_
"""

from google.colab import drive
drive.mount('/content/drive')

cd 255_proj (1)/

ls

!pip install contractions regex

import pandas as pd 
import tensorflow as tf
from tensorflow import keras
import numpy as np 
import matplotlib.pyplot as plt
import os

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder as lencoder
from tensorflow.keras.layers import Embedding
from tensorflow.keras.utils import to_categorical
from sklearn.utils import shuffle
import seaborn as sns
import contractions
from tqdm import tqdm
import nltk
import regex as re

"""Load Dataset"""

data = pd.read_json('News_Category_Dataset_v2 (1).json', lines=True)

data.head()

"""we have remove unwanted columns and  **headline** and **short description**"""

data.drop(['authors', 'link', 'date'], axis = 1, inplace = True)
data['news'] = data[['headline', 'short_description']].agg(' '.join, axis=1)

data.head()

"""Here WORLDPOST and THE WORLDPOST represent same. so we changed THE WORLDPOST to WORLDPOST and Removal of categories which have less than 3000 data points in the data set."""

data.category = data.category.map(lambda x: "WORLDPOST" if x == "THE WORLDPOST" else x)
data = data[data['category'].map(data['category'].value_counts()) > 3000]

data.category.value_counts()

# Shuffle data
df = shuffle(data)
df.reset_index(inplace=True, drop=True)

"""Preprocess the text data"""

clean_reviews=[]
for i in tqdm(df['news']):
    #Rremoving the html tags 
    i=re.sub('(<[\w\s]*/?>)',"",i)
    #Expanding  the contractions 
    i=contractions.fix(i)
    #Removing the special characters
    i=re.sub('[^a-zA-Z0-9\s]+',"",i)
    #Removing  the digits
    i=re.sub('\d+',"",i)
    i=i.lower()
    #converting the text to be of lower case and remvoing the stopwords and words of length less than 3
    #clean_reviews.append(" ".join([j.lower() for j in i.split() if j not in stopwords and len(j)>=3]))
    clean_reviews.append(i)
df.news = clean_reviews

"""Removal to textual data containing less than Six tokens."""

data['words_length'] = data.news.apply(lambda i: len(i.split(" ")))
data = data[data.words_length >= 5]

vocab_size =20000
max_length = 35
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"
num_words=200000
embeddings_dim=100

le = lencoder()
labels = le.fit_transform(df['category'])
labels= to_categorical(labels,num_classes=21)

X,Y = df['news'],df['category']
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(X)

word_index = tokenizer.word_index

X = tokenizer.texts_to_sequences(X)
X = pad_sequences(X, maxlen= max_length, padding=padding_type, truncating=trunc_type)

X_train, X_val1, y_train, y_val1 = train_test_split(X, labels, test_size=0.2, random_state=42,stratify=labels)
X_val, X_test , y_val, y_test= train_test_split(X_val1,y_val1, test_size=0.5, random_state=42, stratify=y_val1)

from keras.layers import Embedding,Conv1D,LSTM,GRU,BatchNormalization,Flatten,Dense
from tensorflow.keras.layers import *
from tensorflow.keras.models import *

# model= Sequential()
# model.add(Embedding(num_words,embeddings,input_length=max_length))

# #model.add(Conv1D(256,10,activation='relu'))
# model.add(keras.layers.Bidirectional(LSTM(256,return_sequences=True)))
# model.add(keras.layers.Dropout(0.4))
# model.add(keras.layers.GRU(256))
# model.add(keras.layers.Dropout(0.4))

# model.add(Dense(64, activation='relu'))
# model.add(keras.layers.Dropout(0.2))

# model.add(Dense(21,activation='softmax'))

model=Sequential()

model.add(Embedding(num_words, embeddings_dim,input_length=max_length))

model.add(LSTM(128,input_shape=(X_train.shape),activation='relu',return_sequences=True))

model.add(Dropout(0.2))

model.add(LSTM(128,activation='relu'))

model.add(Dropout(0.2))

model.add(Dense(32,activation='relu'))

model.add(Dropout(0.2))

model.add(Dense(21,activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

print(model.summary())

# model= Sequential()
# model.add(Embedding(num_words,embeddings,input_length=max_length))
# model.add(Flatten())
# model.add(Dense(64, activation='relu'))
# model.add(keras.layers.Dropout(0.2))

# model.add(Dense(64, activation='relu'))
# model.add(keras.layers.Dropout(0.2))

# model.add(Dense(64, activation='relu'))
# model.add(keras.layers.Dropout(0.2))

# model.add(Dense(64, activation='relu'))
# model.add(keras.layers.Dropout(0.2))

# model.add(Dense(32, activation='relu'))
# model.add(keras.layers.Dropout(0.2))

# model.add(Dense(21,activation='softmax'))

model.summary()

opt = keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss="categorical_crossentropy",
              optimizer=opt,
              metrics=['accuracy']
             )

import tensorflow as tf
tf.config.list_physical_devices('GPU')

early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss', 
                                            patience=2, min_delta=0.0001)

history = model.fit( X_train,y_train,validation_data=(X_val,y_val), epochs=10, batch_size=32,steps_per_epoch=len(X_train)//32, validation_steps = len(X_val)//32, callbacks=early_stop)

print(history.history.keys())

history.history['val_loss']

score, acc = model.evaluate(X_val1, y_val1)



